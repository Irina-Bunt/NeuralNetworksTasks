{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\3741198026.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\3741198026.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\1098455523.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradients are different at (0, 2). Analytic: -0.32202, Numeric: -0.09980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\2688850719.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\2688850719.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\2688850719.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\2688850719.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\3211748656.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\3211748656.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "C:\\Users\\Irina\\AppData\\Local\\Temp\\ipykernel_1788\\3211748656.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1.267633\n",
      "Epoch 1, loss: 1.214758\n",
      "Epoch 2, loss: 1.185835\n",
      "Epoch 3, loss: 1.170005\n",
      "Epoch 4, loss: 1.161352\n",
      "Epoch 5, loss: 1.156615\n",
      "Epoch 6, loss: 1.154026\n",
      "Epoch 7, loss: 1.152607\n",
      "Epoch 8, loss: 1.151832\n",
      "Epoch 9, loss: 1.151402\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x217bfdce400>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.123\n",
      "Epoch 0, loss: 1.151176\n",
      "Epoch 1, loss: 1.151048\n",
      "Epoch 2, loss: 1.150976\n",
      "Epoch 3, loss: 1.150939\n",
      "Epoch 4, loss: 1.150921\n",
      "Epoch 5, loss: 1.150907\n",
      "Epoch 6, loss: 1.150902\n",
      "Epoch 7, loss: 1.150897\n",
      "Epoch 8, loss: 1.150897\n",
      "Epoch 9, loss: 1.150897\n",
      "Epoch 10, loss: 1.150896\n",
      "Epoch 11, loss: 1.150895\n",
      "Epoch 12, loss: 1.150895\n",
      "Epoch 13, loss: 1.150895\n",
      "Epoch 14, loss: 1.150895\n",
      "Epoch 15, loss: 1.150899\n",
      "Epoch 16, loss: 1.150894\n",
      "Epoch 17, loss: 1.150894\n",
      "Epoch 18, loss: 1.150890\n",
      "Epoch 19, loss: 1.150891\n",
      "Epoch 20, loss: 1.150896\n",
      "Epoch 21, loss: 1.150895\n",
      "Epoch 22, loss: 1.150896\n",
      "Epoch 23, loss: 1.150889\n",
      "Epoch 24, loss: 1.150897\n",
      "Epoch 25, loss: 1.150893\n",
      "Epoch 26, loss: 1.150894\n",
      "Epoch 27, loss: 1.150894\n",
      "Epoch 28, loss: 1.150893\n",
      "Epoch 29, loss: 1.150890\n",
      "Epoch 30, loss: 1.150892\n",
      "Epoch 31, loss: 1.150895\n",
      "Epoch 32, loss: 1.150895\n",
      "Epoch 33, loss: 1.150895\n",
      "Epoch 34, loss: 1.150897\n",
      "Epoch 35, loss: 1.150892\n",
      "Epoch 36, loss: 1.150892\n",
      "Epoch 37, loss: 1.150896\n",
      "Epoch 38, loss: 1.150895\n",
      "Epoch 39, loss: 1.150895\n",
      "Epoch 40, loss: 1.150896\n",
      "Epoch 41, loss: 1.150893\n",
      "Epoch 42, loss: 1.150894\n",
      "Epoch 43, loss: 1.150893\n",
      "Epoch 44, loss: 1.150894\n",
      "Epoch 45, loss: 1.150900\n",
      "Epoch 46, loss: 1.150895\n",
      "Epoch 47, loss: 1.150893\n",
      "Epoch 48, loss: 1.150898\n",
      "Epoch 49, loss: 1.150894\n",
      "Epoch 50, loss: 1.150894\n",
      "Epoch 51, loss: 1.150892\n",
      "Epoch 52, loss: 1.150895\n",
      "Epoch 53, loss: 1.150890\n",
      "Epoch 54, loss: 1.150894\n",
      "Epoch 55, loss: 1.150892\n",
      "Epoch 56, loss: 1.150896\n",
      "Epoch 57, loss: 1.150895\n",
      "Epoch 58, loss: 1.150895\n",
      "Epoch 59, loss: 1.150896\n",
      "Epoch 60, loss: 1.150897\n",
      "Epoch 61, loss: 1.150896\n",
      "Epoch 62, loss: 1.150895\n",
      "Epoch 63, loss: 1.150893\n",
      "Epoch 64, loss: 1.150896\n",
      "Epoch 65, loss: 1.150898\n",
      "Epoch 66, loss: 1.150891\n",
      "Epoch 67, loss: 1.150900\n",
      "Epoch 68, loss: 1.150896\n",
      "Epoch 69, loss: 1.150896\n",
      "Epoch 70, loss: 1.150894\n",
      "Epoch 71, loss: 1.150894\n",
      "Epoch 72, loss: 1.150896\n",
      "Epoch 73, loss: 1.150894\n",
      "Epoch 74, loss: 1.150896\n",
      "Epoch 75, loss: 1.150891\n",
      "Epoch 76, loss: 1.150896\n",
      "Epoch 77, loss: 1.150892\n",
      "Epoch 78, loss: 1.150896\n",
      "Epoch 79, loss: 1.150892\n",
      "Epoch 80, loss: 1.150896\n",
      "Epoch 81, loss: 1.150896\n",
      "Epoch 82, loss: 1.150896\n",
      "Epoch 83, loss: 1.150897\n",
      "Epoch 84, loss: 1.150899\n",
      "Epoch 85, loss: 1.150892\n",
      "Epoch 86, loss: 1.150900\n",
      "Epoch 87, loss: 1.150893\n",
      "Epoch 88, loss: 1.150894\n",
      "Epoch 89, loss: 1.150891\n",
      "Epoch 90, loss: 1.150899\n",
      "Epoch 91, loss: 1.150896\n",
      "Epoch 92, loss: 1.150889\n",
      "Epoch 93, loss: 1.150899\n",
      "Epoch 94, loss: 1.150894\n",
      "Epoch 95, loss: 1.150903\n",
      "Epoch 96, loss: 1.150893\n",
      "Epoch 97, loss: 1.150895\n",
      "Epoch 98, loss: 1.150893\n",
      "Epoch 99, loss: 1.150901\n",
      "Accuracy after training for 100 epochs:  0.118\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1.151425\n",
      "Epoch 1, loss: 1.151156\n",
      "Epoch 2, loss: 1.150903\n",
      "Epoch 3, loss: 1.150651\n",
      "Epoch 4, loss: 1.150406\n",
      "Epoch 5, loss: 1.150167\n",
      "Epoch 6, loss: 1.149929\n",
      "Epoch 7, loss: 1.149695\n",
      "Epoch 8, loss: 1.149465\n",
      "Epoch 9, loss: 1.149233\n",
      "Epoch 10, loss: 1.149011\n",
      "Epoch 11, loss: 1.148786\n",
      "Epoch 12, loss: 1.148564\n",
      "Epoch 13, loss: 1.148344\n",
      "Epoch 14, loss: 1.148124\n",
      "Epoch 15, loss: 1.147906\n",
      "Epoch 16, loss: 1.147692\n",
      "Epoch 17, loss: 1.147472\n",
      "Epoch 18, loss: 1.147261\n",
      "Epoch 19, loss: 1.147048\n",
      "Epoch 20, loss: 1.146841\n",
      "Epoch 21, loss: 1.146623\n",
      "Epoch 22, loss: 1.146414\n",
      "Epoch 23, loss: 1.146202\n",
      "Epoch 24, loss: 1.146000\n",
      "Epoch 25, loss: 1.145792\n",
      "Epoch 26, loss: 1.145586\n",
      "Epoch 27, loss: 1.145375\n",
      "Epoch 28, loss: 1.145172\n",
      "Epoch 29, loss: 1.144969\n",
      "Epoch 30, loss: 1.144774\n",
      "Epoch 31, loss: 1.144567\n",
      "Epoch 32, loss: 1.144363\n",
      "Epoch 33, loss: 1.144162\n",
      "Epoch 34, loss: 1.143965\n",
      "Epoch 35, loss: 1.143768\n",
      "Epoch 36, loss: 1.143574\n",
      "Epoch 37, loss: 1.143371\n",
      "Epoch 38, loss: 1.143178\n",
      "Epoch 39, loss: 1.142980\n",
      "Epoch 40, loss: 1.142784\n",
      "Epoch 41, loss: 1.142592\n",
      "Epoch 42, loss: 1.142398\n",
      "Epoch 43, loss: 1.142200\n",
      "Epoch 44, loss: 1.142017\n",
      "Epoch 45, loss: 1.141820\n",
      "Epoch 46, loss: 1.141630\n",
      "Epoch 47, loss: 1.141442\n",
      "Epoch 48, loss: 1.141253\n",
      "Epoch 49, loss: 1.141064\n",
      "Epoch 50, loss: 1.140875\n",
      "Epoch 51, loss: 1.140693\n",
      "Epoch 52, loss: 1.140504\n",
      "Epoch 53, loss: 1.140322\n",
      "Epoch 54, loss: 1.140137\n",
      "Epoch 55, loss: 1.139952\n",
      "Epoch 56, loss: 1.139768\n",
      "Epoch 57, loss: 1.139586\n",
      "Epoch 58, loss: 1.139402\n",
      "Epoch 59, loss: 1.139221\n",
      "Epoch 60, loss: 1.139042\n",
      "Epoch 61, loss: 1.138862\n",
      "Epoch 62, loss: 1.138685\n",
      "Epoch 63, loss: 1.138504\n",
      "Epoch 64, loss: 1.138323\n",
      "Epoch 65, loss: 1.138147\n",
      "Epoch 66, loss: 1.137971\n",
      "Epoch 67, loss: 1.137797\n",
      "Epoch 68, loss: 1.137619\n",
      "Epoch 69, loss: 1.137439\n",
      "Epoch 70, loss: 1.137266\n",
      "Epoch 71, loss: 1.137098\n",
      "Epoch 72, loss: 1.136923\n",
      "Epoch 73, loss: 1.136752\n",
      "Epoch 74, loss: 1.136580\n",
      "Epoch 75, loss: 1.136407\n",
      "Epoch 76, loss: 1.136231\n",
      "Epoch 77, loss: 1.136065\n",
      "Epoch 78, loss: 1.135898\n",
      "Epoch 79, loss: 1.135731\n",
      "Epoch 80, loss: 1.135563\n",
      "Epoch 81, loss: 1.135394\n",
      "Epoch 82, loss: 1.135222\n",
      "Epoch 83, loss: 1.135064\n",
      "Epoch 84, loss: 1.134894\n",
      "Epoch 85, loss: 1.134730\n",
      "Epoch 86, loss: 1.134563\n",
      "Epoch 87, loss: 1.134399\n",
      "Epoch 88, loss: 1.134237\n",
      "Epoch 89, loss: 1.134073\n",
      "Epoch 90, loss: 1.133907\n",
      "Epoch 91, loss: 1.133745\n",
      "Epoch 92, loss: 1.133591\n",
      "Epoch 93, loss: 1.133428\n",
      "Epoch 94, loss: 1.133265\n",
      "Epoch 95, loss: 1.133106\n",
      "Epoch 96, loss: 1.132946\n",
      "Epoch 97, loss: 1.132789\n",
      "Epoch 98, loss: 1.132632\n",
      "Epoch 99, loss: 1.132470\n",
      "Epoch 100, loss: 1.132313\n",
      "Epoch 101, loss: 1.132159\n",
      "Epoch 102, loss: 1.132004\n",
      "Epoch 103, loss: 1.131844\n",
      "Epoch 104, loss: 1.131687\n",
      "Epoch 105, loss: 1.131531\n",
      "Epoch 106, loss: 1.131383\n",
      "Epoch 107, loss: 1.131225\n",
      "Epoch 108, loss: 1.131074\n",
      "Epoch 109, loss: 1.130924\n",
      "Epoch 110, loss: 1.130771\n",
      "Epoch 111, loss: 1.130619\n",
      "Epoch 112, loss: 1.130465\n",
      "Epoch 113, loss: 1.130316\n",
      "Epoch 114, loss: 1.130169\n",
      "Epoch 115, loss: 1.130015\n",
      "Epoch 116, loss: 1.129869\n",
      "Epoch 117, loss: 1.129717\n",
      "Epoch 118, loss: 1.129568\n",
      "Epoch 119, loss: 1.129425\n",
      "Epoch 120, loss: 1.129276\n",
      "Epoch 121, loss: 1.129124\n",
      "Epoch 122, loss: 1.128983\n",
      "Epoch 123, loss: 1.128837\n",
      "Epoch 124, loss: 1.128689\n",
      "Epoch 125, loss: 1.128543\n",
      "Epoch 126, loss: 1.128401\n",
      "Epoch 127, loss: 1.128251\n",
      "Epoch 128, loss: 1.128110\n",
      "Epoch 129, loss: 1.127971\n",
      "Epoch 130, loss: 1.127824\n",
      "Epoch 131, loss: 1.127682\n",
      "Epoch 132, loss: 1.127538\n",
      "Epoch 133, loss: 1.127402\n",
      "Epoch 134, loss: 1.127258\n",
      "Epoch 135, loss: 1.127114\n",
      "Epoch 136, loss: 1.126975\n",
      "Epoch 137, loss: 1.126835\n",
      "Epoch 138, loss: 1.126701\n",
      "Epoch 139, loss: 1.126559\n",
      "Epoch 140, loss: 1.126417\n",
      "Epoch 141, loss: 1.126282\n",
      "Epoch 142, loss: 1.126142\n",
      "Epoch 143, loss: 1.126007\n",
      "Epoch 144, loss: 1.125871\n",
      "Epoch 145, loss: 1.125734\n",
      "Epoch 146, loss: 1.125595\n",
      "Epoch 147, loss: 1.125462\n",
      "Epoch 148, loss: 1.125325\n",
      "Epoch 149, loss: 1.125194\n",
      "Epoch 150, loss: 1.125062\n",
      "Epoch 151, loss: 1.124920\n",
      "Epoch 152, loss: 1.124790\n",
      "Epoch 153, loss: 1.124657\n",
      "Epoch 154, loss: 1.124527\n",
      "Epoch 155, loss: 1.124391\n",
      "Epoch 156, loss: 1.124261\n",
      "Epoch 157, loss: 1.124125\n",
      "Epoch 158, loss: 1.123992\n",
      "Epoch 159, loss: 1.123866\n",
      "Epoch 160, loss: 1.123733\n",
      "Epoch 161, loss: 1.123607\n",
      "Epoch 162, loss: 1.123476\n",
      "Epoch 163, loss: 1.123345\n",
      "Epoch 164, loss: 1.123218\n",
      "Epoch 165, loss: 1.123090\n",
      "Epoch 166, loss: 1.122959\n",
      "Epoch 167, loss: 1.122831\n",
      "Epoch 168, loss: 1.122703\n",
      "Epoch 169, loss: 1.122586\n",
      "Epoch 170, loss: 1.122452\n",
      "Epoch 171, loss: 1.122321\n",
      "Epoch 172, loss: 1.122196\n",
      "Epoch 173, loss: 1.122073\n",
      "Epoch 174, loss: 1.121949\n",
      "Epoch 175, loss: 1.121826\n",
      "Epoch 176, loss: 1.121700\n",
      "Epoch 177, loss: 1.121577\n",
      "Epoch 178, loss: 1.121452\n",
      "Epoch 179, loss: 1.121328\n",
      "Epoch 180, loss: 1.121206\n",
      "Epoch 181, loss: 1.121088\n",
      "Epoch 182, loss: 1.120959\n",
      "Epoch 183, loss: 1.120838\n",
      "Epoch 184, loss: 1.120717\n",
      "Epoch 185, loss: 1.120598\n",
      "Epoch 186, loss: 1.120478\n",
      "Epoch 187, loss: 1.120357\n",
      "Epoch 188, loss: 1.120237\n",
      "Epoch 189, loss: 1.120113\n",
      "Epoch 190, loss: 1.120000\n",
      "Epoch 191, loss: 1.119880\n",
      "Epoch 192, loss: 1.119756\n",
      "Epoch 193, loss: 1.119643\n",
      "Epoch 194, loss: 1.119528\n",
      "Epoch 195, loss: 1.119408\n",
      "Epoch 196, loss: 1.119286\n",
      "Epoch 197, loss: 1.119172\n",
      "Epoch 198, loss: 1.119053\n",
      "Epoch 199, loss: 1.118938\n",
      "Epoch 0, loss: 1.151198\n",
      "Epoch 1, loss: 1.150922\n",
      "Epoch 2, loss: 1.150659\n",
      "Epoch 3, loss: 1.150395\n",
      "Epoch 4, loss: 1.150148\n",
      "Epoch 5, loss: 1.149901\n",
      "Epoch 6, loss: 1.149659\n",
      "Epoch 7, loss: 1.149427\n",
      "Epoch 8, loss: 1.149193\n",
      "Epoch 9, loss: 1.148961\n",
      "Epoch 10, loss: 1.148737\n",
      "Epoch 11, loss: 1.148509\n",
      "Epoch 12, loss: 1.148289\n",
      "Epoch 13, loss: 1.148070\n",
      "Epoch 14, loss: 1.147849\n",
      "Epoch 15, loss: 1.147631\n",
      "Epoch 16, loss: 1.147419\n",
      "Epoch 17, loss: 1.147205\n",
      "Epoch 18, loss: 1.146990\n",
      "Epoch 19, loss: 1.146773\n",
      "Epoch 20, loss: 1.146566\n",
      "Epoch 21, loss: 1.146353\n",
      "Epoch 22, loss: 1.146145\n",
      "Epoch 23, loss: 1.145935\n",
      "Epoch 24, loss: 1.145726\n",
      "Epoch 25, loss: 1.145522\n",
      "Epoch 26, loss: 1.145317\n",
      "Epoch 27, loss: 1.145110\n",
      "Epoch 28, loss: 1.144910\n",
      "Epoch 29, loss: 1.144705\n",
      "Epoch 30, loss: 1.144501\n",
      "Epoch 31, loss: 1.144305\n",
      "Epoch 32, loss: 1.144102\n",
      "Epoch 33, loss: 1.143900\n",
      "Epoch 34, loss: 1.143704\n",
      "Epoch 35, loss: 1.143503\n",
      "Epoch 36, loss: 1.143309\n",
      "Epoch 37, loss: 1.143114\n",
      "Epoch 38, loss: 1.142917\n",
      "Epoch 39, loss: 1.142721\n",
      "Epoch 40, loss: 1.142530\n",
      "Epoch 41, loss: 1.142332\n",
      "Epoch 42, loss: 1.142143\n",
      "Epoch 43, loss: 1.141953\n",
      "Epoch 44, loss: 1.141760\n",
      "Epoch 45, loss: 1.141566\n",
      "Epoch 46, loss: 1.141377\n",
      "Epoch 47, loss: 1.141192\n",
      "Epoch 48, loss: 1.140999\n",
      "Epoch 49, loss: 1.140810\n",
      "Epoch 50, loss: 1.140626\n",
      "Epoch 51, loss: 1.140442\n",
      "Epoch 52, loss: 1.140254\n",
      "Epoch 53, loss: 1.140072\n",
      "Epoch 54, loss: 1.139889\n",
      "Epoch 55, loss: 1.139703\n",
      "Epoch 56, loss: 1.139522\n",
      "Epoch 57, loss: 1.139338\n",
      "Epoch 58, loss: 1.139159\n",
      "Epoch 59, loss: 1.138977\n",
      "Epoch 60, loss: 1.138795\n",
      "Epoch 61, loss: 1.138615\n",
      "Epoch 62, loss: 1.138441\n",
      "Epoch 63, loss: 1.138260\n",
      "Epoch 64, loss: 1.138081\n",
      "Epoch 65, loss: 1.137911\n",
      "Epoch 66, loss: 1.137731\n",
      "Epoch 67, loss: 1.137554\n",
      "Epoch 68, loss: 1.137377\n",
      "Epoch 69, loss: 1.137205\n",
      "Epoch 70, loss: 1.137028\n",
      "Epoch 71, loss: 1.136858\n",
      "Epoch 72, loss: 1.136689\n",
      "Epoch 73, loss: 1.136516\n",
      "Epoch 74, loss: 1.136344\n",
      "Epoch 75, loss: 1.136175\n",
      "Epoch 76, loss: 1.136002\n",
      "Epoch 77, loss: 1.135837\n",
      "Epoch 78, loss: 1.135665\n",
      "Epoch 79, loss: 1.135501\n",
      "Epoch 80, loss: 1.135333\n",
      "Epoch 81, loss: 1.135165\n",
      "Epoch 82, loss: 1.134996\n",
      "Epoch 83, loss: 1.134828\n",
      "Epoch 84, loss: 1.134663\n",
      "Epoch 85, loss: 1.134498\n",
      "Epoch 86, loss: 1.134336\n",
      "Epoch 87, loss: 1.134173\n",
      "Epoch 88, loss: 1.134012\n",
      "Epoch 89, loss: 1.133846\n",
      "Epoch 90, loss: 1.133684\n",
      "Epoch 91, loss: 1.133521\n",
      "Epoch 92, loss: 1.133364\n",
      "Epoch 93, loss: 1.133199\n",
      "Epoch 94, loss: 1.133042\n",
      "Epoch 95, loss: 1.132881\n",
      "Epoch 96, loss: 1.132723\n",
      "Epoch 97, loss: 1.132561\n",
      "Epoch 98, loss: 1.132404\n",
      "Epoch 99, loss: 1.132250\n",
      "Epoch 100, loss: 1.132094\n",
      "Epoch 101, loss: 1.131939\n",
      "Epoch 102, loss: 1.131780\n",
      "Epoch 103, loss: 1.131628\n",
      "Epoch 104, loss: 1.131471\n",
      "Epoch 105, loss: 1.131319\n",
      "Epoch 106, loss: 1.131168\n",
      "Epoch 107, loss: 1.131010\n",
      "Epoch 108, loss: 1.130856\n",
      "Epoch 109, loss: 1.130705\n",
      "Epoch 110, loss: 1.130550\n",
      "Epoch 111, loss: 1.130405\n",
      "Epoch 112, loss: 1.130252\n",
      "Epoch 113, loss: 1.130101\n",
      "Epoch 114, loss: 1.129949\n",
      "Epoch 115, loss: 1.129801\n",
      "Epoch 116, loss: 1.129655\n",
      "Epoch 117, loss: 1.129503\n",
      "Epoch 118, loss: 1.129357\n",
      "Epoch 119, loss: 1.129206\n",
      "Epoch 120, loss: 1.129063\n",
      "Epoch 121, loss: 1.128915\n",
      "Epoch 122, loss: 1.128771\n",
      "Epoch 123, loss: 1.128623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, loss: 1.128483\n",
      "Epoch 125, loss: 1.128336\n",
      "Epoch 126, loss: 1.128190\n",
      "Epoch 127, loss: 1.128047\n",
      "Epoch 128, loss: 1.127906\n",
      "Epoch 129, loss: 1.127759\n",
      "Epoch 130, loss: 1.127612\n",
      "Epoch 131, loss: 1.127472\n",
      "Epoch 132, loss: 1.127333\n",
      "Epoch 133, loss: 1.127194\n",
      "Epoch 134, loss: 1.127053\n",
      "Epoch 135, loss: 1.126915\n",
      "Epoch 136, loss: 1.126770\n",
      "Epoch 137, loss: 1.126629\n",
      "Epoch 138, loss: 1.126492\n",
      "Epoch 139, loss: 1.126356\n",
      "Epoch 140, loss: 1.126219\n",
      "Epoch 141, loss: 1.126078\n",
      "Epoch 142, loss: 1.125940\n",
      "Epoch 143, loss: 1.125805\n",
      "Epoch 144, loss: 1.125666\n",
      "Epoch 145, loss: 1.125536\n",
      "Epoch 146, loss: 1.125401\n",
      "Epoch 147, loss: 1.125259\n",
      "Epoch 148, loss: 1.125126\n",
      "Epoch 149, loss: 1.124991\n",
      "Epoch 150, loss: 1.124858\n",
      "Epoch 151, loss: 1.124723\n",
      "Epoch 152, loss: 1.124593\n",
      "Epoch 153, loss: 1.124459\n",
      "Epoch 154, loss: 1.124322\n",
      "Epoch 155, loss: 1.124194\n",
      "Epoch 156, loss: 1.124062\n",
      "Epoch 157, loss: 1.123929\n",
      "Epoch 158, loss: 1.123800\n",
      "Epoch 159, loss: 1.123673\n",
      "Epoch 160, loss: 1.123542\n",
      "Epoch 161, loss: 1.123411\n",
      "Epoch 162, loss: 1.123279\n",
      "Epoch 163, loss: 1.123153\n",
      "Epoch 164, loss: 1.123027\n",
      "Epoch 165, loss: 1.122894\n",
      "Epoch 166, loss: 1.122767\n",
      "Epoch 167, loss: 1.122640\n",
      "Epoch 168, loss: 1.122512\n",
      "Epoch 169, loss: 1.122386\n",
      "Epoch 170, loss: 1.122262\n",
      "Epoch 171, loss: 1.122137\n",
      "Epoch 172, loss: 1.122008\n",
      "Epoch 173, loss: 1.121882\n",
      "Epoch 174, loss: 1.121758\n",
      "Epoch 175, loss: 1.121631\n",
      "Epoch 176, loss: 1.121513\n",
      "Epoch 177, loss: 1.121393\n",
      "Epoch 178, loss: 1.121263\n",
      "Epoch 179, loss: 1.121140\n",
      "Epoch 180, loss: 1.121019\n",
      "Epoch 181, loss: 1.120898\n",
      "Epoch 182, loss: 1.120771\n",
      "Epoch 183, loss: 1.120654\n",
      "Epoch 184, loss: 1.120533\n",
      "Epoch 185, loss: 1.120409\n",
      "Epoch 186, loss: 1.120293\n",
      "Epoch 187, loss: 1.120170\n",
      "Epoch 188, loss: 1.120053\n",
      "Epoch 189, loss: 1.119930\n",
      "Epoch 190, loss: 1.119815\n",
      "Epoch 191, loss: 1.119694\n",
      "Epoch 192, loss: 1.119577\n",
      "Epoch 193, loss: 1.119459\n",
      "Epoch 194, loss: 1.119340\n",
      "Epoch 195, loss: 1.119220\n",
      "Epoch 196, loss: 1.119107\n",
      "Epoch 197, loss: 1.118987\n",
      "Epoch 198, loss: 1.118875\n",
      "Epoch 199, loss: 1.118756\n",
      "Epoch 0, loss: 1.151190\n",
      "Epoch 1, loss: 1.150915\n",
      "Epoch 2, loss: 1.150648\n",
      "Epoch 3, loss: 1.150397\n",
      "Epoch 4, loss: 1.150146\n",
      "Epoch 5, loss: 1.149906\n",
      "Epoch 6, loss: 1.149659\n",
      "Epoch 7, loss: 1.149428\n",
      "Epoch 8, loss: 1.149199\n",
      "Epoch 9, loss: 1.148964\n",
      "Epoch 10, loss: 1.148740\n",
      "Epoch 11, loss: 1.148516\n",
      "Epoch 12, loss: 1.148295\n",
      "Epoch 13, loss: 1.148077\n",
      "Epoch 14, loss: 1.147860\n",
      "Epoch 15, loss: 1.147644\n",
      "Epoch 16, loss: 1.147424\n",
      "Epoch 17, loss: 1.147210\n",
      "Epoch 18, loss: 1.146995\n",
      "Epoch 19, loss: 1.146784\n",
      "Epoch 20, loss: 1.146570\n",
      "Epoch 21, loss: 1.146361\n",
      "Epoch 22, loss: 1.146153\n",
      "Epoch 23, loss: 1.145949\n",
      "Epoch 24, loss: 1.145742\n",
      "Epoch 25, loss: 1.145531\n",
      "Epoch 26, loss: 1.145324\n",
      "Epoch 27, loss: 1.145124\n",
      "Epoch 28, loss: 1.144923\n",
      "Epoch 29, loss: 1.144720\n",
      "Epoch 30, loss: 1.144518\n",
      "Epoch 31, loss: 1.144318\n",
      "Epoch 32, loss: 1.144114\n",
      "Epoch 33, loss: 1.143913\n",
      "Epoch 34, loss: 1.143715\n",
      "Epoch 35, loss: 1.143522\n",
      "Epoch 36, loss: 1.143327\n",
      "Epoch 37, loss: 1.143126\n",
      "Epoch 38, loss: 1.142936\n",
      "Epoch 39, loss: 1.142742\n",
      "Epoch 40, loss: 1.142544\n",
      "Epoch 41, loss: 1.142352\n",
      "Epoch 42, loss: 1.142160\n",
      "Epoch 43, loss: 1.141973\n",
      "Epoch 44, loss: 1.141777\n",
      "Epoch 45, loss: 1.141589\n",
      "Epoch 46, loss: 1.141401\n",
      "Epoch 47, loss: 1.141210\n",
      "Epoch 48, loss: 1.141023\n",
      "Epoch 49, loss: 1.140833\n",
      "Epoch 50, loss: 1.140649\n",
      "Epoch 51, loss: 1.140461\n",
      "Epoch 52, loss: 1.140279\n",
      "Epoch 53, loss: 1.140096\n",
      "Epoch 54, loss: 1.139912\n",
      "Epoch 55, loss: 1.139724\n",
      "Epoch 56, loss: 1.139546\n",
      "Epoch 57, loss: 1.139365\n",
      "Epoch 58, loss: 1.139183\n",
      "Epoch 59, loss: 1.138999\n",
      "Epoch 60, loss: 1.138821\n",
      "Epoch 61, loss: 1.138641\n",
      "Epoch 62, loss: 1.138466\n",
      "Epoch 63, loss: 1.138287\n",
      "Epoch 64, loss: 1.138107\n",
      "Epoch 65, loss: 1.137933\n",
      "Epoch 66, loss: 1.137754\n",
      "Epoch 67, loss: 1.137586\n",
      "Epoch 68, loss: 1.137404\n",
      "Epoch 69, loss: 1.137232\n",
      "Epoch 70, loss: 1.137060\n",
      "Epoch 71, loss: 1.136888\n",
      "Epoch 72, loss: 1.136719\n",
      "Epoch 73, loss: 1.136541\n",
      "Epoch 74, loss: 1.136372\n",
      "Epoch 75, loss: 1.136200\n",
      "Epoch 76, loss: 1.136031\n",
      "Epoch 77, loss: 1.135863\n",
      "Epoch 78, loss: 1.135694\n",
      "Epoch 79, loss: 1.135528\n",
      "Epoch 80, loss: 1.135362\n",
      "Epoch 81, loss: 1.135190\n",
      "Epoch 82, loss: 1.135025\n",
      "Epoch 83, loss: 1.134864\n",
      "Epoch 84, loss: 1.134696\n",
      "Epoch 85, loss: 1.134531\n",
      "Epoch 86, loss: 1.134364\n",
      "Epoch 87, loss: 1.134202\n",
      "Epoch 88, loss: 1.134043\n",
      "Epoch 89, loss: 1.133879\n",
      "Epoch 90, loss: 1.133714\n",
      "Epoch 91, loss: 1.133552\n",
      "Epoch 92, loss: 1.133392\n",
      "Epoch 93, loss: 1.133230\n",
      "Epoch 94, loss: 1.133077\n",
      "Epoch 95, loss: 1.132913\n",
      "Epoch 96, loss: 1.132756\n",
      "Epoch 97, loss: 1.132599\n",
      "Epoch 98, loss: 1.132436\n",
      "Epoch 99, loss: 1.132277\n",
      "Epoch 100, loss: 1.132132\n",
      "Epoch 101, loss: 1.131967\n",
      "Epoch 102, loss: 1.131813\n",
      "Epoch 103, loss: 1.131656\n",
      "Epoch 104, loss: 1.131505\n",
      "Epoch 105, loss: 1.131351\n",
      "Epoch 106, loss: 1.131196\n",
      "Epoch 107, loss: 1.131047\n",
      "Epoch 108, loss: 1.130889\n",
      "Epoch 109, loss: 1.130737\n",
      "Epoch 110, loss: 1.130589\n",
      "Epoch 111, loss: 1.130434\n",
      "Epoch 112, loss: 1.130287\n",
      "Epoch 113, loss: 1.130137\n",
      "Epoch 114, loss: 1.129983\n",
      "Epoch 115, loss: 1.129836\n",
      "Epoch 116, loss: 1.129686\n",
      "Epoch 117, loss: 1.129534\n",
      "Epoch 118, loss: 1.129396\n",
      "Epoch 119, loss: 1.129244\n",
      "Epoch 120, loss: 1.129098\n",
      "Epoch 121, loss: 1.128948\n",
      "Epoch 122, loss: 1.128802\n",
      "Epoch 123, loss: 1.128658\n",
      "Epoch 124, loss: 1.128513\n",
      "Epoch 125, loss: 1.128366\n",
      "Epoch 126, loss: 1.128226\n",
      "Epoch 127, loss: 1.128080\n",
      "Epoch 128, loss: 1.127939\n",
      "Epoch 129, loss: 1.127794\n",
      "Epoch 130, loss: 1.127655\n",
      "Epoch 131, loss: 1.127511\n",
      "Epoch 132, loss: 1.127368\n",
      "Epoch 133, loss: 1.127229\n",
      "Epoch 134, loss: 1.127086\n",
      "Epoch 135, loss: 1.126950\n",
      "Epoch 136, loss: 1.126808\n",
      "Epoch 137, loss: 1.126662\n",
      "Epoch 138, loss: 1.126526\n",
      "Epoch 139, loss: 1.126390\n",
      "Epoch 140, loss: 1.126252\n",
      "Epoch 141, loss: 1.126114\n",
      "Epoch 142, loss: 1.125978\n",
      "Epoch 143, loss: 1.125838\n",
      "Epoch 144, loss: 1.125706\n",
      "Epoch 145, loss: 1.125566\n",
      "Epoch 146, loss: 1.125430\n",
      "Epoch 147, loss: 1.125295\n",
      "Epoch 148, loss: 1.125161\n",
      "Epoch 149, loss: 1.125030\n",
      "Epoch 150, loss: 1.124892\n",
      "Epoch 151, loss: 1.124758\n",
      "Epoch 152, loss: 1.124629\n",
      "Epoch 153, loss: 1.124495\n",
      "Epoch 154, loss: 1.124360\n",
      "Epoch 155, loss: 1.124232\n",
      "Epoch 156, loss: 1.124104\n",
      "Epoch 157, loss: 1.123966\n",
      "Epoch 158, loss: 1.123839\n",
      "Epoch 159, loss: 1.123708\n",
      "Epoch 160, loss: 1.123575\n",
      "Epoch 161, loss: 1.123446\n",
      "Epoch 162, loss: 1.123314\n",
      "Epoch 163, loss: 1.123190\n",
      "Epoch 164, loss: 1.123062\n",
      "Epoch 165, loss: 1.122933\n",
      "Epoch 166, loss: 1.122804\n",
      "Epoch 167, loss: 1.122671\n",
      "Epoch 168, loss: 1.122550\n",
      "Epoch 169, loss: 1.122419\n",
      "Epoch 170, loss: 1.122297\n",
      "Epoch 171, loss: 1.122174\n",
      "Epoch 172, loss: 1.122043\n",
      "Epoch 173, loss: 1.121920\n",
      "Epoch 174, loss: 1.121797\n",
      "Epoch 175, loss: 1.121671\n",
      "Epoch 176, loss: 1.121549\n",
      "Epoch 177, loss: 1.121426\n",
      "Epoch 178, loss: 1.121300\n",
      "Epoch 179, loss: 1.121174\n",
      "Epoch 180, loss: 1.121054\n",
      "Epoch 181, loss: 1.120932\n",
      "Epoch 182, loss: 1.120809\n",
      "Epoch 183, loss: 1.120690\n",
      "Epoch 184, loss: 1.120566\n",
      "Epoch 185, loss: 1.120449\n",
      "Epoch 186, loss: 1.120327\n",
      "Epoch 187, loss: 1.120207\n",
      "Epoch 188, loss: 1.120087\n",
      "Epoch 189, loss: 1.119969\n",
      "Epoch 190, loss: 1.119850\n",
      "Epoch 191, loss: 1.119732\n",
      "Epoch 192, loss: 1.119609\n",
      "Epoch 193, loss: 1.119489\n",
      "Epoch 194, loss: 1.119381\n",
      "Epoch 195, loss: 1.119258\n",
      "Epoch 196, loss: 1.119141\n",
      "Epoch 197, loss: 1.119025\n",
      "Epoch 198, loss: 1.118911\n",
      "Epoch 199, loss: 1.118793\n",
      "Epoch 0, loss: 1.151408\n",
      "Epoch 1, loss: 1.151379\n",
      "Epoch 2, loss: 1.151350\n",
      "Epoch 3, loss: 1.151322\n",
      "Epoch 4, loss: 1.151294\n",
      "Epoch 5, loss: 1.151266\n",
      "Epoch 6, loss: 1.151239\n",
      "Epoch 7, loss: 1.151211\n",
      "Epoch 8, loss: 1.151183\n",
      "Epoch 9, loss: 1.151155\n",
      "Epoch 10, loss: 1.151128\n",
      "Epoch 11, loss: 1.151101\n",
      "Epoch 12, loss: 1.151073\n",
      "Epoch 13, loss: 1.151047\n",
      "Epoch 14, loss: 1.151019\n",
      "Epoch 15, loss: 1.150993\n",
      "Epoch 16, loss: 1.150966\n",
      "Epoch 17, loss: 1.150939\n",
      "Epoch 18, loss: 1.150913\n",
      "Epoch 19, loss: 1.150886\n",
      "Epoch 20, loss: 1.150859\n",
      "Epoch 21, loss: 1.150833\n",
      "Epoch 22, loss: 1.150807\n",
      "Epoch 23, loss: 1.150780\n",
      "Epoch 24, loss: 1.150755\n",
      "Epoch 25, loss: 1.150728\n",
      "Epoch 26, loss: 1.150702\n",
      "Epoch 27, loss: 1.150676\n",
      "Epoch 28, loss: 1.150651\n",
      "Epoch 29, loss: 1.150625\n",
      "Epoch 30, loss: 1.150599\n",
      "Epoch 31, loss: 1.150574\n",
      "Epoch 32, loss: 1.150548\n",
      "Epoch 33, loss: 1.150523\n",
      "Epoch 34, loss: 1.150497\n",
      "Epoch 35, loss: 1.150472\n",
      "Epoch 36, loss: 1.150447\n",
      "Epoch 37, loss: 1.150422\n",
      "Epoch 38, loss: 1.150397\n",
      "Epoch 39, loss: 1.150372\n",
      "Epoch 40, loss: 1.150347\n",
      "Epoch 41, loss: 1.150322\n",
      "Epoch 42, loss: 1.150296\n",
      "Epoch 43, loss: 1.150271\n",
      "Epoch 44, loss: 1.150247\n",
      "Epoch 45, loss: 1.150222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, loss: 1.150197\n",
      "Epoch 47, loss: 1.150173\n",
      "Epoch 48, loss: 1.150148\n",
      "Epoch 49, loss: 1.150124\n",
      "Epoch 50, loss: 1.150099\n",
      "Epoch 51, loss: 1.150075\n",
      "Epoch 52, loss: 1.150051\n",
      "Epoch 53, loss: 1.150026\n",
      "Epoch 54, loss: 1.150002\n",
      "Epoch 55, loss: 1.149978\n",
      "Epoch 56, loss: 1.149953\n",
      "Epoch 57, loss: 1.149929\n",
      "Epoch 58, loss: 1.149905\n",
      "Epoch 59, loss: 1.149881\n",
      "Epoch 60, loss: 1.149858\n",
      "Epoch 61, loss: 1.149834\n",
      "Epoch 62, loss: 1.149809\n",
      "Epoch 63, loss: 1.149786\n",
      "Epoch 64, loss: 1.149762\n",
      "Epoch 65, loss: 1.149738\n",
      "Epoch 66, loss: 1.149714\n",
      "Epoch 67, loss: 1.149690\n",
      "Epoch 68, loss: 1.149667\n",
      "Epoch 69, loss: 1.149643\n",
      "Epoch 70, loss: 1.149620\n",
      "Epoch 71, loss: 1.149596\n",
      "Epoch 72, loss: 1.149572\n",
      "Epoch 73, loss: 1.149549\n",
      "Epoch 74, loss: 1.149525\n",
      "Epoch 75, loss: 1.149502\n",
      "Epoch 76, loss: 1.149479\n",
      "Epoch 77, loss: 1.149455\n",
      "Epoch 78, loss: 1.149432\n",
      "Epoch 79, loss: 1.149409\n",
      "Epoch 80, loss: 1.149385\n",
      "Epoch 81, loss: 1.149362\n",
      "Epoch 82, loss: 1.149339\n",
      "Epoch 83, loss: 1.149315\n",
      "Epoch 84, loss: 1.149293\n",
      "Epoch 85, loss: 1.149270\n",
      "Epoch 86, loss: 1.149247\n",
      "Epoch 87, loss: 1.149224\n",
      "Epoch 88, loss: 1.149201\n",
      "Epoch 89, loss: 1.149177\n",
      "Epoch 90, loss: 1.149154\n",
      "Epoch 91, loss: 1.149132\n",
      "Epoch 92, loss: 1.149108\n",
      "Epoch 93, loss: 1.149085\n",
      "Epoch 94, loss: 1.149063\n",
      "Epoch 95, loss: 1.149040\n",
      "Epoch 96, loss: 1.149018\n",
      "Epoch 97, loss: 1.148994\n",
      "Epoch 98, loss: 1.148972\n",
      "Epoch 99, loss: 1.148948\n",
      "Epoch 100, loss: 1.148926\n",
      "Epoch 101, loss: 1.148904\n",
      "Epoch 102, loss: 1.148881\n",
      "Epoch 103, loss: 1.148858\n",
      "Epoch 104, loss: 1.148836\n",
      "Epoch 105, loss: 1.148813\n",
      "Epoch 106, loss: 1.148790\n",
      "Epoch 107, loss: 1.148768\n",
      "Epoch 108, loss: 1.148745\n",
      "Epoch 109, loss: 1.148722\n",
      "Epoch 110, loss: 1.148700\n",
      "Epoch 111, loss: 1.148678\n",
      "Epoch 112, loss: 1.148655\n",
      "Epoch 113, loss: 1.148633\n",
      "Epoch 114, loss: 1.148610\n",
      "Epoch 115, loss: 1.148588\n",
      "Epoch 116, loss: 1.148566\n",
      "Epoch 117, loss: 1.148543\n",
      "Epoch 118, loss: 1.148521\n",
      "Epoch 119, loss: 1.148498\n",
      "Epoch 120, loss: 1.148476\n",
      "Epoch 121, loss: 1.148453\n",
      "Epoch 122, loss: 1.148432\n",
      "Epoch 123, loss: 1.148409\n",
      "Epoch 124, loss: 1.148387\n",
      "Epoch 125, loss: 1.148365\n",
      "Epoch 126, loss: 1.148343\n",
      "Epoch 127, loss: 1.148320\n",
      "Epoch 128, loss: 1.148299\n",
      "Epoch 129, loss: 1.148277\n",
      "Epoch 130, loss: 1.148254\n",
      "Epoch 131, loss: 1.148232\n",
      "Epoch 132, loss: 1.148210\n",
      "Epoch 133, loss: 1.148188\n",
      "Epoch 134, loss: 1.148166\n",
      "Epoch 135, loss: 1.148144\n",
      "Epoch 136, loss: 1.148122\n",
      "Epoch 137, loss: 1.148100\n",
      "Epoch 138, loss: 1.148078\n",
      "Epoch 139, loss: 1.148056\n",
      "Epoch 140, loss: 1.148034\n",
      "Epoch 141, loss: 1.148012\n",
      "Epoch 142, loss: 1.147990\n",
      "Epoch 143, loss: 1.147968\n",
      "Epoch 144, loss: 1.147946\n",
      "Epoch 145, loss: 1.147925\n",
      "Epoch 146, loss: 1.147902\n",
      "Epoch 147, loss: 1.147880\n",
      "Epoch 148, loss: 1.147859\n",
      "Epoch 149, loss: 1.147837\n",
      "Epoch 150, loss: 1.147815\n",
      "Epoch 151, loss: 1.147793\n",
      "Epoch 152, loss: 1.147772\n",
      "Epoch 153, loss: 1.147750\n",
      "Epoch 154, loss: 1.147728\n",
      "Epoch 155, loss: 1.147706\n",
      "Epoch 156, loss: 1.147685\n",
      "Epoch 157, loss: 1.147663\n",
      "Epoch 158, loss: 1.147641\n",
      "Epoch 159, loss: 1.147620\n",
      "Epoch 160, loss: 1.147598\n",
      "Epoch 161, loss: 1.147576\n",
      "Epoch 162, loss: 1.147555\n",
      "Epoch 163, loss: 1.147533\n",
      "Epoch 164, loss: 1.147511\n",
      "Epoch 165, loss: 1.147490\n",
      "Epoch 166, loss: 1.147468\n",
      "Epoch 167, loss: 1.147447\n",
      "Epoch 168, loss: 1.147425\n",
      "Epoch 169, loss: 1.147403\n",
      "Epoch 170, loss: 1.147382\n",
      "Epoch 171, loss: 1.147360\n",
      "Epoch 172, loss: 1.147338\n",
      "Epoch 173, loss: 1.147318\n",
      "Epoch 174, loss: 1.147296\n",
      "Epoch 175, loss: 1.147275\n",
      "Epoch 176, loss: 1.147253\n",
      "Epoch 177, loss: 1.147231\n",
      "Epoch 178, loss: 1.147210\n",
      "Epoch 179, loss: 1.147188\n",
      "Epoch 180, loss: 1.147167\n",
      "Epoch 181, loss: 1.147146\n",
      "Epoch 182, loss: 1.147124\n",
      "Epoch 183, loss: 1.147103\n",
      "Epoch 184, loss: 1.147081\n",
      "Epoch 185, loss: 1.147060\n",
      "Epoch 186, loss: 1.147039\n",
      "Epoch 187, loss: 1.147017\n",
      "Epoch 188, loss: 1.146996\n",
      "Epoch 189, loss: 1.146974\n",
      "Epoch 190, loss: 1.146953\n",
      "Epoch 191, loss: 1.146932\n",
      "Epoch 192, loss: 1.146911\n",
      "Epoch 193, loss: 1.146890\n",
      "Epoch 194, loss: 1.146868\n",
      "Epoch 195, loss: 1.146847\n",
      "Epoch 196, loss: 1.146826\n",
      "Epoch 197, loss: 1.146804\n",
      "Epoch 198, loss: 1.146783\n",
      "Epoch 199, loss: 1.146762\n",
      "Epoch 0, loss: 1.151291\n",
      "Epoch 1, loss: 1.151260\n",
      "Epoch 2, loss: 1.151230\n",
      "Epoch 3, loss: 1.151201\n",
      "Epoch 4, loss: 1.151172\n",
      "Epoch 5, loss: 1.151142\n",
      "Epoch 6, loss: 1.151112\n",
      "Epoch 7, loss: 1.151083\n",
      "Epoch 8, loss: 1.151054\n",
      "Epoch 9, loss: 1.151025\n",
      "Epoch 10, loss: 1.150996\n",
      "Epoch 11, loss: 1.150967\n",
      "Epoch 12, loss: 1.150939\n",
      "Epoch 13, loss: 1.150910\n",
      "Epoch 14, loss: 1.150882\n",
      "Epoch 15, loss: 1.150854\n",
      "Epoch 16, loss: 1.150825\n",
      "Epoch 17, loss: 1.150798\n",
      "Epoch 18, loss: 1.150770\n",
      "Epoch 19, loss: 1.150743\n",
      "Epoch 20, loss: 1.150715\n",
      "Epoch 21, loss: 1.150687\n",
      "Epoch 22, loss: 1.150660\n",
      "Epoch 23, loss: 1.150633\n",
      "Epoch 24, loss: 1.150606\n",
      "Epoch 25, loss: 1.150579\n",
      "Epoch 26, loss: 1.150552\n",
      "Epoch 27, loss: 1.150525\n",
      "Epoch 28, loss: 1.150498\n",
      "Epoch 29, loss: 1.150471\n",
      "Epoch 30, loss: 1.150445\n",
      "Epoch 31, loss: 1.150418\n",
      "Epoch 32, loss: 1.150392\n",
      "Epoch 33, loss: 1.150366\n",
      "Epoch 34, loss: 1.150339\n",
      "Epoch 35, loss: 1.150314\n",
      "Epoch 36, loss: 1.150287\n",
      "Epoch 37, loss: 1.150261\n",
      "Epoch 38, loss: 1.150236\n",
      "Epoch 39, loss: 1.150210\n",
      "Epoch 40, loss: 1.150185\n",
      "Epoch 41, loss: 1.150159\n",
      "Epoch 42, loss: 1.150133\n",
      "Epoch 43, loss: 1.150108\n",
      "Epoch 44, loss: 1.150082\n",
      "Epoch 45, loss: 1.150057\n",
      "Epoch 46, loss: 1.150032\n",
      "Epoch 47, loss: 1.150007\n",
      "Epoch 48, loss: 1.149982\n",
      "Epoch 49, loss: 1.149956\n",
      "Epoch 50, loss: 1.149931\n",
      "Epoch 51, loss: 1.149906\n",
      "Epoch 52, loss: 1.149882\n",
      "Epoch 53, loss: 1.149857\n",
      "Epoch 54, loss: 1.149832\n",
      "Epoch 55, loss: 1.149808\n",
      "Epoch 56, loss: 1.149783\n",
      "Epoch 57, loss: 1.149758\n",
      "Epoch 58, loss: 1.149734\n",
      "Epoch 59, loss: 1.149710\n",
      "Epoch 60, loss: 1.149685\n",
      "Epoch 61, loss: 1.149660\n",
      "Epoch 62, loss: 1.149636\n",
      "Epoch 63, loss: 1.149612\n",
      "Epoch 64, loss: 1.149588\n",
      "Epoch 65, loss: 1.149564\n",
      "Epoch 66, loss: 1.149539\n",
      "Epoch 67, loss: 1.149516\n",
      "Epoch 68, loss: 1.149492\n",
      "Epoch 69, loss: 1.149468\n",
      "Epoch 70, loss: 1.149444\n",
      "Epoch 71, loss: 1.149420\n",
      "Epoch 72, loss: 1.149396\n",
      "Epoch 73, loss: 1.149372\n",
      "Epoch 74, loss: 1.149348\n",
      "Epoch 75, loss: 1.149324\n",
      "Epoch 76, loss: 1.149301\n",
      "Epoch 77, loss: 1.149278\n",
      "Epoch 78, loss: 1.149253\n",
      "Epoch 79, loss: 1.149231\n",
      "Epoch 80, loss: 1.149207\n",
      "Epoch 81, loss: 1.149184\n",
      "Epoch 82, loss: 1.149160\n",
      "Epoch 83, loss: 1.149137\n",
      "Epoch 84, loss: 1.149114\n",
      "Epoch 85, loss: 1.149090\n",
      "Epoch 86, loss: 1.149067\n",
      "Epoch 87, loss: 1.149044\n",
      "Epoch 88, loss: 1.149020\n",
      "Epoch 89, loss: 1.148997\n",
      "Epoch 90, loss: 1.148974\n",
      "Epoch 91, loss: 1.148951\n",
      "Epoch 92, loss: 1.148928\n",
      "Epoch 93, loss: 1.148905\n",
      "Epoch 94, loss: 1.148882\n",
      "Epoch 95, loss: 1.148859\n",
      "Epoch 96, loss: 1.148836\n",
      "Epoch 97, loss: 1.148814\n",
      "Epoch 98, loss: 1.148790\n",
      "Epoch 99, loss: 1.148767\n",
      "Epoch 100, loss: 1.148745\n",
      "Epoch 101, loss: 1.148722\n",
      "Epoch 102, loss: 1.148699\n",
      "Epoch 103, loss: 1.148676\n",
      "Epoch 104, loss: 1.148654\n",
      "Epoch 105, loss: 1.148631\n",
      "Epoch 106, loss: 1.148609\n",
      "Epoch 107, loss: 1.148586\n",
      "Epoch 108, loss: 1.148563\n",
      "Epoch 109, loss: 1.148541\n",
      "Epoch 110, loss: 1.148518\n",
      "Epoch 111, loss: 1.148496\n",
      "Epoch 112, loss: 1.148473\n",
      "Epoch 113, loss: 1.148450\n",
      "Epoch 114, loss: 1.148428\n",
      "Epoch 115, loss: 1.148406\n",
      "Epoch 116, loss: 1.148384\n",
      "Epoch 117, loss: 1.148361\n",
      "Epoch 118, loss: 1.148339\n",
      "Epoch 119, loss: 1.148316\n",
      "Epoch 120, loss: 1.148294\n",
      "Epoch 121, loss: 1.148272\n",
      "Epoch 122, loss: 1.148249\n",
      "Epoch 123, loss: 1.148227\n",
      "Epoch 124, loss: 1.148205\n",
      "Epoch 125, loss: 1.148183\n",
      "Epoch 126, loss: 1.148161\n",
      "Epoch 127, loss: 1.148139\n",
      "Epoch 128, loss: 1.148116\n",
      "Epoch 129, loss: 1.148094\n",
      "Epoch 130, loss: 1.148072\n",
      "Epoch 131, loss: 1.148050\n",
      "Epoch 132, loss: 1.148028\n",
      "Epoch 133, loss: 1.148006\n",
      "Epoch 134, loss: 1.147984\n",
      "Epoch 135, loss: 1.147962\n",
      "Epoch 136, loss: 1.147940\n",
      "Epoch 137, loss: 1.147918\n",
      "Epoch 138, loss: 1.147896\n",
      "Epoch 139, loss: 1.147874\n",
      "Epoch 140, loss: 1.147852\n",
      "Epoch 141, loss: 1.147830\n",
      "Epoch 142, loss: 1.147808\n",
      "Epoch 143, loss: 1.147786\n",
      "Epoch 144, loss: 1.147765\n",
      "Epoch 145, loss: 1.147743\n",
      "Epoch 146, loss: 1.147720\n",
      "Epoch 147, loss: 1.147699\n",
      "Epoch 148, loss: 1.147677\n",
      "Epoch 149, loss: 1.147656\n",
      "Epoch 150, loss: 1.147634\n",
      "Epoch 151, loss: 1.147612\n",
      "Epoch 152, loss: 1.147590\n",
      "Epoch 153, loss: 1.147568\n",
      "Epoch 154, loss: 1.147547\n",
      "Epoch 155, loss: 1.147526\n",
      "Epoch 156, loss: 1.147504\n",
      "Epoch 157, loss: 1.147482\n",
      "Epoch 158, loss: 1.147460\n",
      "Epoch 159, loss: 1.147439\n",
      "Epoch 160, loss: 1.147417\n",
      "Epoch 161, loss: 1.147396\n",
      "Epoch 162, loss: 1.147374\n",
      "Epoch 163, loss: 1.147352\n",
      "Epoch 164, loss: 1.147331\n",
      "Epoch 165, loss: 1.147309\n",
      "Epoch 166, loss: 1.147288\n",
      "Epoch 167, loss: 1.147266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168, loss: 1.147245\n",
      "Epoch 169, loss: 1.147224\n",
      "Epoch 170, loss: 1.147202\n",
      "Epoch 171, loss: 1.147180\n",
      "Epoch 172, loss: 1.147159\n",
      "Epoch 173, loss: 1.147138\n",
      "Epoch 174, loss: 1.147116\n",
      "Epoch 175, loss: 1.147095\n",
      "Epoch 176, loss: 1.147073\n",
      "Epoch 177, loss: 1.147052\n",
      "Epoch 178, loss: 1.147031\n",
      "Epoch 179, loss: 1.147009\n",
      "Epoch 180, loss: 1.146988\n",
      "Epoch 181, loss: 1.146966\n",
      "Epoch 182, loss: 1.146945\n",
      "Epoch 183, loss: 1.146923\n",
      "Epoch 184, loss: 1.146903\n",
      "Epoch 185, loss: 1.146881\n",
      "Epoch 186, loss: 1.146861\n",
      "Epoch 187, loss: 1.146839\n",
      "Epoch 188, loss: 1.146818\n",
      "Epoch 189, loss: 1.146797\n",
      "Epoch 190, loss: 1.146775\n",
      "Epoch 191, loss: 1.146754\n",
      "Epoch 192, loss: 1.146732\n",
      "Epoch 193, loss: 1.146711\n",
      "Epoch 194, loss: 1.146690\n",
      "Epoch 195, loss: 1.146669\n",
      "Epoch 196, loss: 1.146648\n",
      "Epoch 197, loss: 1.146627\n",
      "Epoch 198, loss: 1.146606\n",
      "Epoch 199, loss: 1.146585\n",
      "Epoch 0, loss: 1.151487\n",
      "Epoch 1, loss: 1.151460\n",
      "Epoch 2, loss: 1.151432\n",
      "Epoch 3, loss: 1.151405\n",
      "Epoch 4, loss: 1.151377\n",
      "Epoch 5, loss: 1.151349\n",
      "Epoch 6, loss: 1.151322\n",
      "Epoch 7, loss: 1.151295\n",
      "Epoch 8, loss: 1.151268\n",
      "Epoch 9, loss: 1.151241\n",
      "Epoch 10, loss: 1.151213\n",
      "Epoch 11, loss: 1.151187\n",
      "Epoch 12, loss: 1.151160\n",
      "Epoch 13, loss: 1.151133\n",
      "Epoch 14, loss: 1.151107\n",
      "Epoch 15, loss: 1.151081\n",
      "Epoch 16, loss: 1.151054\n",
      "Epoch 17, loss: 1.151028\n",
      "Epoch 18, loss: 1.151002\n",
      "Epoch 19, loss: 1.150976\n",
      "Epoch 20, loss: 1.150949\n",
      "Epoch 21, loss: 1.150923\n",
      "Epoch 22, loss: 1.150897\n",
      "Epoch 23, loss: 1.150872\n",
      "Epoch 24, loss: 1.150847\n",
      "Epoch 25, loss: 1.150821\n",
      "Epoch 26, loss: 1.150794\n",
      "Epoch 27, loss: 1.150769\n",
      "Epoch 28, loss: 1.150744\n",
      "Epoch 29, loss: 1.150718\n",
      "Epoch 30, loss: 1.150693\n",
      "Epoch 31, loss: 1.150668\n",
      "Epoch 32, loss: 1.150642\n",
      "Epoch 33, loss: 1.150617\n",
      "Epoch 34, loss: 1.150592\n",
      "Epoch 35, loss: 1.150568\n",
      "Epoch 36, loss: 1.150543\n",
      "Epoch 37, loss: 1.150517\n",
      "Epoch 38, loss: 1.150492\n",
      "Epoch 39, loss: 1.150468\n",
      "Epoch 40, loss: 1.150443\n",
      "Epoch 41, loss: 1.150419\n",
      "Epoch 42, loss: 1.150394\n",
      "Epoch 43, loss: 1.150369\n",
      "Epoch 44, loss: 1.150345\n",
      "Epoch 45, loss: 1.150320\n",
      "Epoch 46, loss: 1.150296\n",
      "Epoch 47, loss: 1.150272\n",
      "Epoch 48, loss: 1.150248\n",
      "Epoch 49, loss: 1.150223\n",
      "Epoch 50, loss: 1.150198\n",
      "Epoch 51, loss: 1.150175\n",
      "Epoch 52, loss: 1.150150\n",
      "Epoch 53, loss: 1.150126\n",
      "Epoch 54, loss: 1.150103\n",
      "Epoch 55, loss: 1.150079\n",
      "Epoch 56, loss: 1.150055\n",
      "Epoch 57, loss: 1.150031\n",
      "Epoch 58, loss: 1.150007\n",
      "Epoch 59, loss: 1.149983\n",
      "Epoch 60, loss: 1.149959\n",
      "Epoch 61, loss: 1.149935\n",
      "Epoch 62, loss: 1.149912\n",
      "Epoch 63, loss: 1.149888\n",
      "Epoch 64, loss: 1.149864\n",
      "Epoch 65, loss: 1.149841\n",
      "Epoch 66, loss: 1.149817\n",
      "Epoch 67, loss: 1.149794\n",
      "Epoch 68, loss: 1.149770\n",
      "Epoch 69, loss: 1.149747\n",
      "Epoch 70, loss: 1.149724\n",
      "Epoch 71, loss: 1.149700\n",
      "Epoch 72, loss: 1.149676\n",
      "Epoch 73, loss: 1.149653\n",
      "Epoch 74, loss: 1.149630\n",
      "Epoch 75, loss: 1.149607\n",
      "Epoch 76, loss: 1.149584\n",
      "Epoch 77, loss: 1.149560\n",
      "Epoch 78, loss: 1.149537\n",
      "Epoch 79, loss: 1.149514\n",
      "Epoch 80, loss: 1.149491\n",
      "Epoch 81, loss: 1.149468\n",
      "Epoch 82, loss: 1.149445\n",
      "Epoch 83, loss: 1.149422\n",
      "Epoch 84, loss: 1.149399\n",
      "Epoch 85, loss: 1.149376\n",
      "Epoch 86, loss: 1.149353\n",
      "Epoch 87, loss: 1.149330\n",
      "Epoch 88, loss: 1.149307\n",
      "Epoch 89, loss: 1.149285\n",
      "Epoch 90, loss: 1.149262\n",
      "Epoch 91, loss: 1.149239\n",
      "Epoch 92, loss: 1.149216\n",
      "Epoch 93, loss: 1.149193\n",
      "Epoch 94, loss: 1.149171\n",
      "Epoch 95, loss: 1.149148\n",
      "Epoch 96, loss: 1.149125\n",
      "Epoch 97, loss: 1.149103\n",
      "Epoch 98, loss: 1.149080\n",
      "Epoch 99, loss: 1.149057\n",
      "Epoch 100, loss: 1.149035\n",
      "Epoch 101, loss: 1.149012\n",
      "Epoch 102, loss: 1.148990\n",
      "Epoch 103, loss: 1.148967\n",
      "Epoch 104, loss: 1.148945\n",
      "Epoch 105, loss: 1.148922\n",
      "Epoch 106, loss: 1.148900\n",
      "Epoch 107, loss: 1.148877\n",
      "Epoch 108, loss: 1.148855\n",
      "Epoch 109, loss: 1.148833\n",
      "Epoch 110, loss: 1.148810\n",
      "Epoch 111, loss: 1.148787\n",
      "Epoch 112, loss: 1.148765\n",
      "Epoch 113, loss: 1.148743\n",
      "Epoch 114, loss: 1.148721\n",
      "Epoch 115, loss: 1.148698\n",
      "Epoch 116, loss: 1.148676\n",
      "Epoch 117, loss: 1.148654\n",
      "Epoch 118, loss: 1.148632\n",
      "Epoch 119, loss: 1.148610\n",
      "Epoch 120, loss: 1.148587\n",
      "Epoch 121, loss: 1.148565\n",
      "Epoch 122, loss: 1.148543\n",
      "Epoch 123, loss: 1.148521\n",
      "Epoch 124, loss: 1.148499\n",
      "Epoch 125, loss: 1.148477\n",
      "Epoch 126, loss: 1.148455\n",
      "Epoch 127, loss: 1.148433\n",
      "Epoch 128, loss: 1.148411\n",
      "Epoch 129, loss: 1.148388\n",
      "Epoch 130, loss: 1.148367\n",
      "Epoch 131, loss: 1.148345\n",
      "Epoch 132, loss: 1.148323\n",
      "Epoch 133, loss: 1.148301\n",
      "Epoch 134, loss: 1.148279\n",
      "Epoch 135, loss: 1.148257\n",
      "Epoch 136, loss: 1.148234\n",
      "Epoch 137, loss: 1.148212\n",
      "Epoch 138, loss: 1.148191\n",
      "Epoch 139, loss: 1.148169\n",
      "Epoch 140, loss: 1.148147\n",
      "Epoch 141, loss: 1.148125\n",
      "Epoch 142, loss: 1.148104\n",
      "Epoch 143, loss: 1.148082\n",
      "Epoch 144, loss: 1.148060\n",
      "Epoch 145, loss: 1.148038\n",
      "Epoch 146, loss: 1.148016\n",
      "Epoch 147, loss: 1.147995\n",
      "Epoch 148, loss: 1.147973\n",
      "Epoch 149, loss: 1.147951\n",
      "Epoch 150, loss: 1.147930\n",
      "Epoch 151, loss: 1.147907\n",
      "Epoch 152, loss: 1.147886\n",
      "Epoch 153, loss: 1.147864\n",
      "Epoch 154, loss: 1.147843\n",
      "Epoch 155, loss: 1.147821\n",
      "Epoch 156, loss: 1.147799\n",
      "Epoch 157, loss: 1.147778\n",
      "Epoch 158, loss: 1.147757\n",
      "Epoch 159, loss: 1.147735\n",
      "Epoch 160, loss: 1.147713\n",
      "Epoch 161, loss: 1.147692\n",
      "Epoch 162, loss: 1.147670\n",
      "Epoch 163, loss: 1.147648\n",
      "Epoch 164, loss: 1.147627\n",
      "Epoch 165, loss: 1.147605\n",
      "Epoch 166, loss: 1.147584\n",
      "Epoch 167, loss: 1.147562\n",
      "Epoch 168, loss: 1.147541\n",
      "Epoch 169, loss: 1.147519\n",
      "Epoch 170, loss: 1.147498\n",
      "Epoch 171, loss: 1.147476\n",
      "Epoch 172, loss: 1.147455\n",
      "Epoch 173, loss: 1.147433\n",
      "Epoch 174, loss: 1.147412\n",
      "Epoch 175, loss: 1.147391\n",
      "Epoch 176, loss: 1.147369\n",
      "Epoch 177, loss: 1.147348\n",
      "Epoch 178, loss: 1.147326\n",
      "Epoch 179, loss: 1.147305\n",
      "Epoch 180, loss: 1.147284\n",
      "Epoch 181, loss: 1.147262\n",
      "Epoch 182, loss: 1.147241\n",
      "Epoch 183, loss: 1.147220\n",
      "Epoch 184, loss: 1.147199\n",
      "Epoch 185, loss: 1.147177\n",
      "Epoch 186, loss: 1.147156\n",
      "Epoch 187, loss: 1.147135\n",
      "Epoch 188, loss: 1.147114\n",
      "Epoch 189, loss: 1.147092\n",
      "Epoch 190, loss: 1.147071\n",
      "Epoch 191, loss: 1.147050\n",
      "Epoch 192, loss: 1.147028\n",
      "Epoch 193, loss: 1.147007\n",
      "Epoch 194, loss: 1.146986\n",
      "Epoch 195, loss: 1.146965\n",
      "Epoch 196, loss: 1.146944\n",
      "Epoch 197, loss: 1.146923\n",
      "Epoch 198, loss: 1.146902\n",
      "Epoch 199, loss: 1.146880\n",
      "Epoch 0, loss: 1.151438\n",
      "Epoch 1, loss: 1.151435\n",
      "Epoch 2, loss: 1.151433\n",
      "Epoch 3, loss: 1.151430\n",
      "Epoch 4, loss: 1.151427\n",
      "Epoch 5, loss: 1.151424\n",
      "Epoch 6, loss: 1.151421\n",
      "Epoch 7, loss: 1.151419\n",
      "Epoch 8, loss: 1.151416\n",
      "Epoch 9, loss: 1.151413\n",
      "Epoch 10, loss: 1.151410\n",
      "Epoch 11, loss: 1.151407\n",
      "Epoch 12, loss: 1.151404\n",
      "Epoch 13, loss: 1.151402\n",
      "Epoch 14, loss: 1.151399\n",
      "Epoch 15, loss: 1.151396\n",
      "Epoch 16, loss: 1.151393\n",
      "Epoch 17, loss: 1.151391\n",
      "Epoch 18, loss: 1.151388\n",
      "Epoch 19, loss: 1.151385\n",
      "Epoch 20, loss: 1.151382\n",
      "Epoch 21, loss: 1.151379\n",
      "Epoch 22, loss: 1.151376\n",
      "Epoch 23, loss: 1.151374\n",
      "Epoch 24, loss: 1.151371\n",
      "Epoch 25, loss: 1.151368\n",
      "Epoch 26, loss: 1.151365\n",
      "Epoch 27, loss: 1.151362\n",
      "Epoch 28, loss: 1.151360\n",
      "Epoch 29, loss: 1.151357\n",
      "Epoch 30, loss: 1.151354\n",
      "Epoch 31, loss: 1.151351\n",
      "Epoch 32, loss: 1.151349\n",
      "Epoch 33, loss: 1.151346\n",
      "Epoch 34, loss: 1.151343\n",
      "Epoch 35, loss: 1.151340\n",
      "Epoch 36, loss: 1.151337\n",
      "Epoch 37, loss: 1.151335\n",
      "Epoch 38, loss: 1.151332\n",
      "Epoch 39, loss: 1.151329\n",
      "Epoch 40, loss: 1.151326\n",
      "Epoch 41, loss: 1.151323\n",
      "Epoch 42, loss: 1.151321\n",
      "Epoch 43, loss: 1.151318\n",
      "Epoch 44, loss: 1.151315\n",
      "Epoch 45, loss: 1.151312\n",
      "Epoch 46, loss: 1.151310\n",
      "Epoch 47, loss: 1.151307\n",
      "Epoch 48, loss: 1.151304\n",
      "Epoch 49, loss: 1.151301\n",
      "Epoch 50, loss: 1.151299\n",
      "Epoch 51, loss: 1.151296\n",
      "Epoch 52, loss: 1.151293\n",
      "Epoch 53, loss: 1.151290\n",
      "Epoch 54, loss: 1.151287\n",
      "Epoch 55, loss: 1.151285\n",
      "Epoch 56, loss: 1.151282\n",
      "Epoch 57, loss: 1.151279\n",
      "Epoch 58, loss: 1.151276\n",
      "Epoch 59, loss: 1.151274\n",
      "Epoch 60, loss: 1.151271\n",
      "Epoch 61, loss: 1.151268\n",
      "Epoch 62, loss: 1.151265\n",
      "Epoch 63, loss: 1.151263\n",
      "Epoch 64, loss: 1.151260\n",
      "Epoch 65, loss: 1.151257\n",
      "Epoch 66, loss: 1.151254\n",
      "Epoch 67, loss: 1.151252\n",
      "Epoch 68, loss: 1.151249\n",
      "Epoch 69, loss: 1.151246\n",
      "Epoch 70, loss: 1.151243\n",
      "Epoch 71, loss: 1.151241\n",
      "Epoch 72, loss: 1.151238\n",
      "Epoch 73, loss: 1.151235\n",
      "Epoch 74, loss: 1.151232\n",
      "Epoch 75, loss: 1.151230\n",
      "Epoch 76, loss: 1.151227\n",
      "Epoch 77, loss: 1.151224\n",
      "Epoch 78, loss: 1.151222\n",
      "Epoch 79, loss: 1.151219\n",
      "Epoch 80, loss: 1.151216\n",
      "Epoch 81, loss: 1.151213\n",
      "Epoch 82, loss: 1.151211\n",
      "Epoch 83, loss: 1.151208\n",
      "Epoch 84, loss: 1.151205\n",
      "Epoch 85, loss: 1.151202\n",
      "Epoch 86, loss: 1.151200\n",
      "Epoch 87, loss: 1.151197\n",
      "Epoch 88, loss: 1.151194\n",
      "Epoch 89, loss: 1.151192\n",
      "Epoch 90, loss: 1.151189\n",
      "Epoch 91, loss: 1.151186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, loss: 1.151183\n",
      "Epoch 93, loss: 1.151181\n",
      "Epoch 94, loss: 1.151178\n",
      "Epoch 95, loss: 1.151175\n",
      "Epoch 96, loss: 1.151172\n",
      "Epoch 97, loss: 1.151170\n",
      "Epoch 98, loss: 1.151167\n",
      "Epoch 99, loss: 1.151164\n",
      "Epoch 100, loss: 1.151162\n",
      "Epoch 101, loss: 1.151159\n",
      "Epoch 102, loss: 1.151156\n",
      "Epoch 103, loss: 1.151153\n",
      "Epoch 104, loss: 1.151151\n",
      "Epoch 105, loss: 1.151148\n",
      "Epoch 106, loss: 1.151145\n",
      "Epoch 107, loss: 1.151143\n",
      "Epoch 108, loss: 1.151140\n",
      "Epoch 109, loss: 1.151137\n",
      "Epoch 110, loss: 1.151134\n",
      "Epoch 111, loss: 1.151132\n",
      "Epoch 112, loss: 1.151129\n",
      "Epoch 113, loss: 1.151126\n",
      "Epoch 114, loss: 1.151124\n",
      "Epoch 115, loss: 1.151121\n",
      "Epoch 116, loss: 1.151118\n",
      "Epoch 117, loss: 1.151116\n",
      "Epoch 118, loss: 1.151113\n",
      "Epoch 119, loss: 1.151110\n",
      "Epoch 120, loss: 1.151107\n",
      "Epoch 121, loss: 1.151105\n",
      "Epoch 122, loss: 1.151102\n",
      "Epoch 123, loss: 1.151099\n",
      "Epoch 124, loss: 1.151097\n",
      "Epoch 125, loss: 1.151094\n",
      "Epoch 126, loss: 1.151091\n",
      "Epoch 127, loss: 1.151089\n",
      "Epoch 128, loss: 1.151086\n",
      "Epoch 129, loss: 1.151083\n",
      "Epoch 130, loss: 1.151081\n",
      "Epoch 131, loss: 1.151078\n",
      "Epoch 132, loss: 1.151075\n",
      "Epoch 133, loss: 1.151072\n",
      "Epoch 134, loss: 1.151070\n",
      "Epoch 135, loss: 1.151067\n",
      "Epoch 136, loss: 1.151064\n",
      "Epoch 137, loss: 1.151062\n",
      "Epoch 138, loss: 1.151059\n",
      "Epoch 139, loss: 1.151056\n",
      "Epoch 140, loss: 1.151054\n",
      "Epoch 141, loss: 1.151051\n",
      "Epoch 142, loss: 1.151048\n",
      "Epoch 143, loss: 1.151046\n",
      "Epoch 144, loss: 1.151043\n",
      "Epoch 145, loss: 1.151040\n",
      "Epoch 146, loss: 1.151038\n",
      "Epoch 147, loss: 1.151035\n",
      "Epoch 148, loss: 1.151032\n",
      "Epoch 149, loss: 1.151030\n",
      "Epoch 150, loss: 1.151027\n",
      "Epoch 151, loss: 1.151024\n",
      "Epoch 152, loss: 1.151022\n",
      "Epoch 153, loss: 1.151019\n",
      "Epoch 154, loss: 1.151016\n",
      "Epoch 155, loss: 1.151014\n",
      "Epoch 156, loss: 1.151011\n",
      "Epoch 157, loss: 1.151008\n",
      "Epoch 158, loss: 1.151006\n",
      "Epoch 159, loss: 1.151003\n",
      "Epoch 160, loss: 1.151000\n",
      "Epoch 161, loss: 1.150998\n",
      "Epoch 162, loss: 1.150995\n",
      "Epoch 163, loss: 1.150992\n",
      "Epoch 164, loss: 1.150990\n",
      "Epoch 165, loss: 1.150987\n",
      "Epoch 166, loss: 1.150984\n",
      "Epoch 167, loss: 1.150982\n",
      "Epoch 168, loss: 1.150979\n",
      "Epoch 169, loss: 1.150976\n",
      "Epoch 170, loss: 1.150974\n",
      "Epoch 171, loss: 1.150971\n",
      "Epoch 172, loss: 1.150969\n",
      "Epoch 173, loss: 1.150966\n",
      "Epoch 174, loss: 1.150963\n",
      "Epoch 175, loss: 1.150961\n",
      "Epoch 176, loss: 1.150958\n",
      "Epoch 177, loss: 1.150955\n",
      "Epoch 178, loss: 1.150953\n",
      "Epoch 179, loss: 1.150950\n",
      "Epoch 180, loss: 1.150947\n",
      "Epoch 181, loss: 1.150945\n",
      "Epoch 182, loss: 1.150942\n",
      "Epoch 183, loss: 1.150939\n",
      "Epoch 184, loss: 1.150937\n",
      "Epoch 185, loss: 1.150934\n",
      "Epoch 186, loss: 1.150932\n",
      "Epoch 187, loss: 1.150929\n",
      "Epoch 188, loss: 1.150926\n",
      "Epoch 189, loss: 1.150924\n",
      "Epoch 190, loss: 1.150921\n",
      "Epoch 191, loss: 1.150918\n",
      "Epoch 192, loss: 1.150916\n",
      "Epoch 193, loss: 1.150913\n",
      "Epoch 194, loss: 1.150910\n",
      "Epoch 195, loss: 1.150908\n",
      "Epoch 196, loss: 1.150905\n",
      "Epoch 197, loss: 1.150903\n",
      "Epoch 198, loss: 1.150900\n",
      "Epoch 199, loss: 1.150897\n",
      "Epoch 0, loss: 1.151480\n",
      "Epoch 1, loss: 1.151478\n",
      "Epoch 2, loss: 1.151475\n",
      "Epoch 3, loss: 1.151472\n",
      "Epoch 4, loss: 1.151469\n",
      "Epoch 5, loss: 1.151467\n",
      "Epoch 6, loss: 1.151464\n",
      "Epoch 7, loss: 1.151461\n",
      "Epoch 8, loss: 1.151458\n",
      "Epoch 9, loss: 1.151456\n",
      "Epoch 10, loss: 1.151453\n",
      "Epoch 11, loss: 1.151450\n",
      "Epoch 12, loss: 1.151447\n",
      "Epoch 13, loss: 1.151445\n",
      "Epoch 14, loss: 1.151442\n",
      "Epoch 15, loss: 1.151439\n",
      "Epoch 16, loss: 1.151436\n",
      "Epoch 17, loss: 1.151434\n",
      "Epoch 18, loss: 1.151431\n",
      "Epoch 19, loss: 1.151428\n",
      "Epoch 20, loss: 1.151425\n",
      "Epoch 21, loss: 1.151423\n",
      "Epoch 22, loss: 1.151420\n",
      "Epoch 23, loss: 1.151417\n",
      "Epoch 24, loss: 1.151414\n",
      "Epoch 25, loss: 1.151412\n",
      "Epoch 26, loss: 1.151409\n",
      "Epoch 27, loss: 1.151406\n",
      "Epoch 28, loss: 1.151403\n",
      "Epoch 29, loss: 1.151401\n",
      "Epoch 30, loss: 1.151398\n",
      "Epoch 31, loss: 1.151395\n",
      "Epoch 32, loss: 1.151392\n",
      "Epoch 33, loss: 1.151390\n",
      "Epoch 34, loss: 1.151387\n",
      "Epoch 35, loss: 1.151384\n",
      "Epoch 36, loss: 1.151382\n",
      "Epoch 37, loss: 1.151379\n",
      "Epoch 38, loss: 1.151376\n",
      "Epoch 39, loss: 1.151373\n",
      "Epoch 40, loss: 1.151371\n",
      "Epoch 41, loss: 1.151368\n",
      "Epoch 42, loss: 1.151365\n",
      "Epoch 43, loss: 1.151362\n",
      "Epoch 44, loss: 1.151360\n",
      "Epoch 45, loss: 1.151357\n",
      "Epoch 46, loss: 1.151354\n",
      "Epoch 47, loss: 1.151352\n",
      "Epoch 48, loss: 1.151349\n",
      "Epoch 49, loss: 1.151346\n",
      "Epoch 50, loss: 1.151343\n",
      "Epoch 51, loss: 1.151341\n",
      "Epoch 52, loss: 1.151338\n",
      "Epoch 53, loss: 1.151335\n",
      "Epoch 54, loss: 1.151333\n",
      "Epoch 55, loss: 1.151330\n",
      "Epoch 56, loss: 1.151327\n",
      "Epoch 57, loss: 1.151324\n",
      "Epoch 58, loss: 1.151322\n",
      "Epoch 59, loss: 1.151319\n",
      "Epoch 60, loss: 1.151316\n",
      "Epoch 61, loss: 1.151314\n",
      "Epoch 62, loss: 1.151311\n",
      "Epoch 63, loss: 1.151308\n",
      "Epoch 64, loss: 1.151306\n",
      "Epoch 65, loss: 1.151303\n",
      "Epoch 66, loss: 1.151300\n",
      "Epoch 67, loss: 1.151297\n",
      "Epoch 68, loss: 1.151295\n",
      "Epoch 69, loss: 1.151292\n",
      "Epoch 70, loss: 1.151289\n",
      "Epoch 71, loss: 1.151287\n",
      "Epoch 72, loss: 1.151284\n",
      "Epoch 73, loss: 1.151281\n",
      "Epoch 74, loss: 1.151279\n",
      "Epoch 75, loss: 1.151276\n",
      "Epoch 76, loss: 1.151273\n",
      "Epoch 77, loss: 1.151271\n",
      "Epoch 78, loss: 1.151268\n",
      "Epoch 79, loss: 1.151265\n",
      "Epoch 80, loss: 1.151262\n",
      "Epoch 81, loss: 1.151260\n",
      "Epoch 82, loss: 1.151257\n",
      "Epoch 83, loss: 1.151254\n",
      "Epoch 84, loss: 1.151252\n",
      "Epoch 85, loss: 1.151249\n",
      "Epoch 86, loss: 1.151246\n",
      "Epoch 87, loss: 1.151244\n",
      "Epoch 88, loss: 1.151241\n",
      "Epoch 89, loss: 1.151238\n",
      "Epoch 90, loss: 1.151236\n",
      "Epoch 91, loss: 1.151233\n",
      "Epoch 92, loss: 1.151230\n",
      "Epoch 93, loss: 1.151228\n",
      "Epoch 94, loss: 1.151225\n",
      "Epoch 95, loss: 1.151222\n",
      "Epoch 96, loss: 1.151220\n",
      "Epoch 97, loss: 1.151217\n",
      "Epoch 98, loss: 1.151214\n",
      "Epoch 99, loss: 1.151212\n",
      "Epoch 100, loss: 1.151209\n",
      "Epoch 101, loss: 1.151206\n",
      "Epoch 102, loss: 1.151204\n",
      "Epoch 103, loss: 1.151201\n",
      "Epoch 104, loss: 1.151198\n",
      "Epoch 105, loss: 1.151196\n",
      "Epoch 106, loss: 1.151193\n",
      "Epoch 107, loss: 1.151190\n",
      "Epoch 108, loss: 1.151188\n",
      "Epoch 109, loss: 1.151185\n",
      "Epoch 110, loss: 1.151182\n",
      "Epoch 111, loss: 1.151180\n",
      "Epoch 112, loss: 1.151177\n",
      "Epoch 113, loss: 1.151174\n",
      "Epoch 114, loss: 1.151172\n",
      "Epoch 115, loss: 1.151169\n",
      "Epoch 116, loss: 1.151166\n",
      "Epoch 117, loss: 1.151164\n",
      "Epoch 118, loss: 1.151161\n",
      "Epoch 119, loss: 1.151158\n",
      "Epoch 120, loss: 1.151156\n",
      "Epoch 121, loss: 1.151153\n",
      "Epoch 122, loss: 1.151150\n",
      "Epoch 123, loss: 1.151148\n",
      "Epoch 124, loss: 1.151145\n",
      "Epoch 125, loss: 1.151143\n",
      "Epoch 126, loss: 1.151140\n",
      "Epoch 127, loss: 1.151137\n",
      "Epoch 128, loss: 1.151135\n",
      "Epoch 129, loss: 1.151132\n",
      "Epoch 130, loss: 1.151129\n",
      "Epoch 131, loss: 1.151127\n",
      "Epoch 132, loss: 1.151124\n",
      "Epoch 133, loss: 1.151121\n",
      "Epoch 134, loss: 1.151119\n",
      "Epoch 135, loss: 1.151116\n",
      "Epoch 136, loss: 1.151113\n",
      "Epoch 137, loss: 1.151111\n",
      "Epoch 138, loss: 1.151108\n",
      "Epoch 139, loss: 1.151106\n",
      "Epoch 140, loss: 1.151103\n",
      "Epoch 141, loss: 1.151100\n",
      "Epoch 142, loss: 1.151098\n",
      "Epoch 143, loss: 1.151095\n",
      "Epoch 144, loss: 1.151092\n",
      "Epoch 145, loss: 1.151090\n",
      "Epoch 146, loss: 1.151087\n",
      "Epoch 147, loss: 1.151085\n",
      "Epoch 148, loss: 1.151082\n",
      "Epoch 149, loss: 1.151079\n",
      "Epoch 150, loss: 1.151077\n",
      "Epoch 151, loss: 1.151074\n",
      "Epoch 152, loss: 1.151071\n",
      "Epoch 153, loss: 1.151069\n",
      "Epoch 154, loss: 1.151066\n",
      "Epoch 155, loss: 1.151064\n",
      "Epoch 156, loss: 1.151061\n",
      "Epoch 157, loss: 1.151058\n",
      "Epoch 158, loss: 1.151056\n",
      "Epoch 159, loss: 1.151053\n",
      "Epoch 160, loss: 1.151050\n",
      "Epoch 161, loss: 1.151048\n",
      "Epoch 162, loss: 1.151045\n",
      "Epoch 163, loss: 1.151043\n",
      "Epoch 164, loss: 1.151040\n",
      "Epoch 165, loss: 1.151037\n",
      "Epoch 166, loss: 1.151035\n",
      "Epoch 167, loss: 1.151032\n",
      "Epoch 168, loss: 1.151030\n",
      "Epoch 169, loss: 1.151027\n",
      "Epoch 170, loss: 1.151024\n",
      "Epoch 171, loss: 1.151022\n",
      "Epoch 172, loss: 1.151019\n",
      "Epoch 173, loss: 1.151017\n",
      "Epoch 174, loss: 1.151014\n",
      "Epoch 175, loss: 1.151011\n",
      "Epoch 176, loss: 1.151009\n",
      "Epoch 177, loss: 1.151006\n",
      "Epoch 178, loss: 1.151004\n",
      "Epoch 179, loss: 1.151001\n",
      "Epoch 180, loss: 1.150998\n",
      "Epoch 181, loss: 1.150996\n",
      "Epoch 182, loss: 1.150993\n",
      "Epoch 183, loss: 1.150990\n",
      "Epoch 184, loss: 1.150988\n",
      "Epoch 185, loss: 1.150985\n",
      "Epoch 186, loss: 1.150983\n",
      "Epoch 187, loss: 1.150980\n",
      "Epoch 188, loss: 1.150978\n",
      "Epoch 189, loss: 1.150975\n",
      "Epoch 190, loss: 1.150972\n",
      "Epoch 191, loss: 1.150970\n",
      "Epoch 192, loss: 1.150967\n",
      "Epoch 193, loss: 1.150965\n",
      "Epoch 194, loss: 1.150962\n",
      "Epoch 195, loss: 1.150959\n",
      "Epoch 196, loss: 1.150957\n",
      "Epoch 197, loss: 1.150954\n",
      "Epoch 198, loss: 1.150952\n",
      "Epoch 199, loss: 1.150949\n",
      "Epoch 0, loss: 1.151194\n",
      "Epoch 1, loss: 1.151191\n",
      "Epoch 2, loss: 1.151189\n",
      "Epoch 3, loss: 1.151186\n",
      "Epoch 4, loss: 1.151183\n",
      "Epoch 5, loss: 1.151180\n",
      "Epoch 6, loss: 1.151178\n",
      "Epoch 7, loss: 1.151175\n",
      "Epoch 8, loss: 1.151172\n",
      "Epoch 9, loss: 1.151169\n",
      "Epoch 10, loss: 1.151167\n",
      "Epoch 11, loss: 1.151164\n",
      "Epoch 12, loss: 1.151161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 1.151158\n",
      "Epoch 14, loss: 1.151156\n",
      "Epoch 15, loss: 1.151153\n",
      "Epoch 16, loss: 1.151150\n",
      "Epoch 17, loss: 1.151147\n",
      "Epoch 18, loss: 1.151145\n",
      "Epoch 19, loss: 1.151142\n",
      "Epoch 20, loss: 1.151139\n",
      "Epoch 21, loss: 1.151136\n",
      "Epoch 22, loss: 1.151134\n",
      "Epoch 23, loss: 1.151131\n",
      "Epoch 24, loss: 1.151128\n",
      "Epoch 25, loss: 1.151125\n",
      "Epoch 26, loss: 1.151123\n",
      "Epoch 27, loss: 1.151120\n",
      "Epoch 28, loss: 1.151117\n",
      "Epoch 29, loss: 1.151115\n",
      "Epoch 30, loss: 1.151112\n",
      "Epoch 31, loss: 1.151109\n",
      "Epoch 32, loss: 1.151106\n",
      "Epoch 33, loss: 1.151104\n",
      "Epoch 34, loss: 1.151101\n",
      "Epoch 35, loss: 1.151098\n",
      "Epoch 36, loss: 1.151095\n",
      "Epoch 37, loss: 1.151093\n",
      "Epoch 38, loss: 1.151090\n",
      "Epoch 39, loss: 1.151087\n",
      "Epoch 40, loss: 1.151085\n",
      "Epoch 41, loss: 1.151082\n",
      "Epoch 42, loss: 1.151079\n",
      "Epoch 43, loss: 1.151076\n",
      "Epoch 44, loss: 1.151074\n",
      "Epoch 45, loss: 1.151071\n",
      "Epoch 46, loss: 1.151068\n",
      "Epoch 47, loss: 1.151065\n",
      "Epoch 48, loss: 1.151063\n",
      "Epoch 49, loss: 1.151060\n",
      "Epoch 50, loss: 1.151057\n",
      "Epoch 51, loss: 1.151055\n",
      "Epoch 52, loss: 1.151052\n",
      "Epoch 53, loss: 1.151049\n",
      "Epoch 54, loss: 1.151047\n",
      "Epoch 55, loss: 1.151044\n",
      "Epoch 56, loss: 1.151041\n",
      "Epoch 57, loss: 1.151038\n",
      "Epoch 58, loss: 1.151036\n",
      "Epoch 59, loss: 1.151033\n",
      "Epoch 60, loss: 1.151030\n",
      "Epoch 61, loss: 1.151028\n",
      "Epoch 62, loss: 1.151025\n",
      "Epoch 63, loss: 1.151022\n",
      "Epoch 64, loss: 1.151019\n",
      "Epoch 65, loss: 1.151017\n",
      "Epoch 66, loss: 1.151014\n",
      "Epoch 67, loss: 1.151011\n",
      "Epoch 68, loss: 1.151009\n",
      "Epoch 69, loss: 1.151006\n",
      "Epoch 70, loss: 1.151003\n",
      "Epoch 71, loss: 1.151001\n",
      "Epoch 72, loss: 1.150998\n",
      "Epoch 73, loss: 1.150995\n",
      "Epoch 74, loss: 1.150993\n",
      "Epoch 75, loss: 1.150990\n",
      "Epoch 76, loss: 1.150987\n",
      "Epoch 77, loss: 1.150985\n",
      "Epoch 78, loss: 1.150982\n",
      "Epoch 79, loss: 1.150979\n",
      "Epoch 80, loss: 1.150976\n",
      "Epoch 81, loss: 1.150974\n",
      "Epoch 82, loss: 1.150971\n",
      "Epoch 83, loss: 1.150968\n",
      "Epoch 84, loss: 1.150966\n",
      "Epoch 85, loss: 1.150963\n",
      "Epoch 86, loss: 1.150960\n",
      "Epoch 87, loss: 1.150958\n",
      "Epoch 88, loss: 1.150955\n",
      "Epoch 89, loss: 1.150952\n",
      "Epoch 90, loss: 1.150950\n",
      "Epoch 91, loss: 1.150947\n",
      "Epoch 92, loss: 1.150944\n",
      "Epoch 93, loss: 1.150942\n",
      "Epoch 94, loss: 1.150939\n",
      "Epoch 95, loss: 1.150936\n",
      "Epoch 96, loss: 1.150934\n",
      "Epoch 97, loss: 1.150931\n",
      "Epoch 98, loss: 1.150928\n",
      "Epoch 99, loss: 1.150926\n",
      "Epoch 100, loss: 1.150923\n",
      "Epoch 101, loss: 1.150920\n",
      "Epoch 102, loss: 1.150918\n",
      "Epoch 103, loss: 1.150915\n",
      "Epoch 104, loss: 1.150912\n",
      "Epoch 105, loss: 1.150910\n",
      "Epoch 106, loss: 1.150907\n",
      "Epoch 107, loss: 1.150904\n",
      "Epoch 108, loss: 1.150902\n",
      "Epoch 109, loss: 1.150899\n",
      "Epoch 110, loss: 1.150896\n",
      "Epoch 111, loss: 1.150894\n",
      "Epoch 112, loss: 1.150891\n",
      "Epoch 113, loss: 1.150888\n",
      "Epoch 114, loss: 1.150886\n",
      "Epoch 115, loss: 1.150883\n",
      "Epoch 116, loss: 1.150881\n",
      "Epoch 117, loss: 1.150878\n",
      "Epoch 118, loss: 1.150875\n",
      "Epoch 119, loss: 1.150873\n",
      "Epoch 120, loss: 1.150870\n",
      "Epoch 121, loss: 1.150867\n",
      "Epoch 122, loss: 1.150865\n",
      "Epoch 123, loss: 1.150862\n",
      "Epoch 124, loss: 1.150859\n",
      "Epoch 125, loss: 1.150857\n",
      "Epoch 126, loss: 1.150854\n",
      "Epoch 127, loss: 1.150851\n",
      "Epoch 128, loss: 1.150849\n",
      "Epoch 129, loss: 1.150846\n",
      "Epoch 130, loss: 1.150844\n",
      "Epoch 131, loss: 1.150841\n",
      "Epoch 132, loss: 1.150838\n",
      "Epoch 133, loss: 1.150836\n",
      "Epoch 134, loss: 1.150833\n",
      "Epoch 135, loss: 1.150830\n",
      "Epoch 136, loss: 1.150828\n",
      "Epoch 137, loss: 1.150825\n",
      "Epoch 138, loss: 1.150822\n",
      "Epoch 139, loss: 1.150820\n",
      "Epoch 140, loss: 1.150817\n",
      "Epoch 141, loss: 1.150815\n",
      "Epoch 142, loss: 1.150812\n",
      "Epoch 143, loss: 1.150809\n",
      "Epoch 144, loss: 1.150807\n",
      "Epoch 145, loss: 1.150804\n",
      "Epoch 146, loss: 1.150801\n",
      "Epoch 147, loss: 1.150799\n",
      "Epoch 148, loss: 1.150796\n",
      "Epoch 149, loss: 1.150794\n",
      "Epoch 150, loss: 1.150791\n",
      "Epoch 151, loss: 1.150788\n",
      "Epoch 152, loss: 1.150786\n",
      "Epoch 153, loss: 1.150783\n",
      "Epoch 154, loss: 1.150780\n",
      "Epoch 155, loss: 1.150778\n",
      "Epoch 156, loss: 1.150775\n",
      "Epoch 157, loss: 1.150773\n",
      "Epoch 158, loss: 1.150770\n",
      "Epoch 159, loss: 1.150767\n",
      "Epoch 160, loss: 1.150765\n",
      "Epoch 161, loss: 1.150762\n",
      "Epoch 162, loss: 1.150760\n",
      "Epoch 163, loss: 1.150757\n",
      "Epoch 164, loss: 1.150754\n",
      "Epoch 165, loss: 1.150752\n",
      "Epoch 166, loss: 1.150749\n",
      "Epoch 167, loss: 1.150747\n",
      "Epoch 168, loss: 1.150744\n",
      "Epoch 169, loss: 1.150741\n",
      "Epoch 170, loss: 1.150739\n",
      "Epoch 171, loss: 1.150736\n",
      "Epoch 172, loss: 1.150733\n",
      "Epoch 173, loss: 1.150731\n",
      "Epoch 174, loss: 1.150728\n",
      "Epoch 175, loss: 1.150726\n",
      "Epoch 176, loss: 1.150723\n",
      "Epoch 177, loss: 1.150721\n",
      "Epoch 178, loss: 1.150718\n",
      "Epoch 179, loss: 1.150715\n",
      "Epoch 180, loss: 1.150713\n",
      "Epoch 181, loss: 1.150710\n",
      "Epoch 182, loss: 1.150708\n",
      "Epoch 183, loss: 1.150705\n",
      "Epoch 184, loss: 1.150702\n",
      "Epoch 185, loss: 1.150700\n",
      "Epoch 186, loss: 1.150697\n",
      "Epoch 187, loss: 1.150695\n",
      "Epoch 188, loss: 1.150692\n",
      "Epoch 189, loss: 1.150689\n",
      "Epoch 190, loss: 1.150687\n",
      "Epoch 191, loss: 1.150684\n",
      "Epoch 192, loss: 1.150682\n",
      "Epoch 193, loss: 1.150679\n",
      "Epoch 194, loss: 1.150676\n",
      "Epoch 195, loss: 1.150674\n",
      "Epoch 196, loss: 1.150671\n",
      "Epoch 197, loss: 1.150669\n",
      "Epoch 198, loss: 1.150666\n",
      "Epoch 199, loss: 1.150664\n",
      "best validation accuracy achieved: 0.218000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                                      learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        \n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        \n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_lr, best_rs = lr, rs\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.190000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
